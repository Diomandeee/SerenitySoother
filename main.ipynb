{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"db/serenity_soother.db\"\n",
    "\n",
    "# get a connection to the database\n",
    "conn = sqlite3.connect(db_name)\n",
    "\n",
    "# get a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# retrieve all the tables in the database\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# print the tables\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "# get the user table\n",
    "scripts = cursor.execute(\"SELECT * FROM scripts;\")\n",
    "scenes = cursor.execute(\"SELECT * FROM scenes;\")\n",
    "scenes = scenes.fetchall()\n",
    "scripts = scripts.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a pandas dataframe from the users\n",
    "scenes_df = pd.DataFrame(scripts, columns=[i[0] for i in cursor.description])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scenes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_df = pd.DataFrame(scripts, columns=[i[0] for i in cursor.description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 11:10:16,195 - DataUploader - ERROR - Failed to load /Users/mohameddiomande/Desktop/1792x1024/data/61/tickets.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_1.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_2.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_3.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_4.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_5.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_6.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_7.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_8.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_9.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_10.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_11.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_12.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_13.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_14.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/audio/7cae9bfd-6687-4922-be64-a90a97506038/segment_15.mp3\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_1.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_2.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_3.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_4.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_5.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_6.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_7.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_8.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_9.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_10.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_11.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_12.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_13.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_14.png\n",
      "Processed /Users/mohameddiomande/Desktop/1792x1024/data/61/image/7cae9bfd-6687-4922-be64-a90a97506038/image_15.png\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/SerenitySoother/app/services/nft_service.py:76\u001b[0m, in \u001b[0;36mDataUploader.loadNFTreeCSV\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     77\u001b[0m         reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(file)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mohameddiomande/Desktop/1792x1024/data/61/tickets.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      5\u001b[0m data_uploader \u001b[38;5;241m=\u001b[39m DataUploader(directory\u001b[38;5;241m=\u001b[39mbase_path)\n\u001b[0;32m----> 6\u001b[0m parsed_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_uploader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_all_media_in_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/mohameddiomande/Desktop/1792x1024/data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print(parsed_data.head())\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/app/services/nft_service.py:437\u001b[0m, in \u001b[0;36mDataUploader.upload_all_media_in_parallel\u001b[0;34m(self, base_path)\u001b[0m\n\u001b[1;32m    436\u001b[0m             os\u001b[38;5;241m.\u001b[39mmakedirs(nftree_root_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 437\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessOneDirectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnftree_root_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_media_data\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/app/services/nft_service.py:164\u001b[0m, in \u001b[0;36mDataUploader.processOneDirectory\u001b[0;34m(self, src_root, dest_root)\u001b[0m\n\u001b[1;32m    163\u001b[0m tickets_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(src_root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtickets.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 164\u001b[0m nft_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadNFTreeCSV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickets_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m children \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(src_root))\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/app/services/nft_service.py:92\u001b[0m, in \u001b[0;36mDataUploader.loadNFTreeCSV\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_out\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFailed to load \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/app/services/nft_service.py:34\u001b[0m, in \u001b[0;36mDataUploader.error_out\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(message)\n\u001b[0;32m---> 34\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2145\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[1;32m   2143\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 2145\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2149\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py:1454\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[0;32m-> 1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py:1345\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1342\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py:1192\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1185\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   1190\u001b[0m ):\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1192\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py:1082\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1080\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[1;32m   1081\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1082\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m   1083\u001b[0m )\n\u001b[1;32m   1085\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1086\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/SerenitySoother/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py:1150\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1150\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[1;32m   1151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "\n",
    "from app.services.nft_service import DataUploader\n",
    "\n",
    "\n",
    "base_path = \"/Users/mohameddiomande/Desktop/1792x1024/data\"\n",
    "data_uploader = DataUploader(directory=base_path)\n",
    "parsed_data = data_uploader.upload_all_media_in_parallel(\n",
    "    \"/Users/mohameddiomande/Desktop/1792x1024/data\"\n",
    ")\n",
    "\n",
    "\n",
    "# print(parsed_data.head())\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "\n",
    "class DataUploader:\n",
    "    # Existing methods...\n",
    "\n",
    "    def convert_to_hf_dataset(self, df):\n",
    "        \"\"\"\n",
    "        Converts a pandas DataFrame to a Hugging Face dataset.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to convert.\n",
    "\n",
    "        Returns:\n",
    "            Dataset: A Hugging Face dataset created from the DataFrame.\n",
    "        \"\"\"\n",
    "        # Convert DataFrame to a Hugging Face Dataset\n",
    "        hf_dataset = Dataset.from_pandas(df)\n",
    "        return hf_dataset\n",
    "\n",
    "    def upload_to_huggingface(self, dataset, dataset_name, save_path):\n",
    "        \"\"\"\n",
    "        Uploads a dataset to Hugging Face Datasets.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The Hugging Face dataset to upload.\n",
    "            dataset_name (str): The name of the dataset to create or update.\n",
    "            save_path (str): The path to save the dataset locally before pushing to Hugging Face.\n",
    "\n",
    "        Returns:\n",
    "            str: A URL or identifier for the uploaded dataset.\n",
    "        \"\"\"\n",
    "        # You must authenticate to Hugging Face Hub (assuming you have HF Hub CLI installed)\n",
    "        # os.system(\"huggingface-cli login\")\n",
    "        \n",
    "        # Save the dataset locally\n",
    "        dataset.save_to_disk(save_path)\n",
    "        \n",
    "        # Push to Hugging Face Datasets (adjust this command according to your authentication and access rights)\n",
    "        # os.system(f\"cd {save_path} && huggingface-cli repo create {dataset_name} --type dataset && git push\")\n",
    "        \n",
    "        # The above step assumes you have your Hugging Face credentials set up\n",
    "        return f\"Dataset uploaded to Hugging Face under the name {dataset_name}\"\n",
    "\n",
    "    def upload_data(self, base_path, dataset_name):\n",
    "        \"\"\"\n",
    "        Processes and uploads data from a given directory to Hugging Face Datasets.\n",
    "\n",
    "        Args:\n",
    "            base_path (str): The base path where the files are located.\n",
    "            dataset_name (str): The name for the dataset to be created on Hugging Face.\n",
    "\n",
    "        Returns:\n",
    "            str: A confirmation message with the dataset URL or name.\n",
    "        \"\"\"\n",
    "        exploded_df = self.explode_list_columns(base_path)\n",
    "        hf_dataset = self.convert_to_hf_dataset(exploded_df)\n",
    "        save_path = f\"./hf_datasets/{dataset_name}\"  # Local save path for the dataset\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        return self.upload_to_huggingface(hf_dataset, dataset_name, save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming parsed_data_df is your DataFrame and the list columns are 'prompt', 'caption', 'image', 'audio'\n",
    "# Create a new DataFrame by applying a lambda function to each row, which converts it into a DataFrame\n",
    "# and then concatenates these DataFrames:\n",
    "parsed_data_df = pd.DataFrame(parsed_data)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to pad lists to the same length\n",
    "def pad_lists_to_max_length(row):\n",
    "    max_length = max(len(row['prompt']), len(row['caption']), len(row['image']), len(row['audio']))\n",
    "    return {\n",
    "        'prompt': row['prompt'] + [None]*(max_length - len(row['prompt'])),\n",
    "        'caption': row['caption'] + [None]*(max_length - len(row['caption'])),\n",
    "        'image': row['image'] + [None]*(max_length - len(row['image'])),\n",
    "        'audio': row['audio'] + [None]*(max_length - len(row['audio']))\n",
    "    }\n",
    "\n",
    "# Apply the padding function and then explode the DataFrame\n",
    "exploded_df = pd.concat(\n",
    "    parsed_data_df.apply(lambda x: pd.DataFrame(pad_lists_to_max_length(x)), axis=1).tolist(), ignore_index=True\n",
    ")\n",
    "\n",
    "# Now check the resulting DataFrame\n",
    "print(exploded_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parsed_data_df = pd.DataFrame(parsed_data)\n",
    "exploded_df = parsed_data_df.explode('prompt')\n",
    "\n",
    "# Assuming alignment, the following columns will align naturally:\n",
    "exploded_df['caption'] = parsed_data_df['caption'].explode()\n",
    "exploded_df['image'] = parsed_data_df['image'].explode()\n",
    "exploded_df['audio'] = parsed_data_df['audio'].explode()\n",
    "\n",
    "# Now check the resulting DataFrame\n",
    "print(exploded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from app.routers.chat import chat_service, generate_prompt\n",
    "from app.services.prompt_generator_service import PromptGenerator\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = PromptGenerator(chat=chat_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data_df  =pd.read_csv(\"data_best.csv\")\n",
    "\n",
    "# combine the prompt and caption columns\n",
    "parsed_data_df['prompt_caption'] = parsed_data_df['prompt'] + \"\\n\\n\" + parsed_data_df['caption']\n",
    "\n",
    "\n",
    "example_pairs = list(zip(parsed_data_df['prompt'], parsed_data_df['caption']))[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = gen.generate_parallel(\n",
    "    num_prompts=20,\n",
    "    example_pairs=example_pairs,\n",
    "    max_workers=4,\n",
    "    generate_prompt=generate_prompt,\n",
    "    batch_size=3\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate = gen.generate_parallel(\n",
    "    num_prompts=20,\n",
    "    example_pairs=example_pairs,\n",
    "    max_workers=1,\n",
    "    generate_prompt=generate_prompt,\n",
    "    batch_size=1    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "\n",
    "def generate_batch_examples(\n",
    "    data: pd.DataFrame,\n",
    "    filename: str = \"batch_requests\",\n",
    "    model: str = \"gpt-3.5-turbo-0125\",\n",
    "    max_tokens: int = 1000,\n",
    "    api_endpoint: str = \"/v1/chat/completions\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate a batch file for API requests from a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing the columns for system, user messages.\n",
    "        filename (str): Name of the file to save the batch requests.\n",
    "        model (str): Model identifier for the API request.\n",
    "        max_tokens (int): Maximum number of tokens for the response.\n",
    "        api_endpoint (str): API endpoint for the request.\n",
    "    \"\"\"\n",
    "    with open(f\"{filename}.jsonl\", \"w\") as file:\n",
    "        for index, row in data.iterrows():\n",
    "            request_data = {\n",
    "                \"custom_id\": f\"request-{index}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": api_endpoint,\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [],\n",
    "                    \"max_tokens\": max_tokens\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Append system message if available\n",
    "            if 'system_message' in row:\n",
    "                request_data['body']['messages'].append({\"role\": \"system\", \"content\": row['system_message']})\n",
    "            \n",
    "            # Append user and assistant messages\n",
    "            if 'user_message' in row and 'assistant_message' in row:\n",
    "                request_data['body']['messages'].append({\"role\": \"user\", \"content\": row['user_message']})\n",
    "                request_data['body']['messages'].append({\"role\": \"assistant\", \"content\": row['assistant_message']})\n",
    "            \n",
    "            file.write(json.dumps(request_data) + \"\\n\")\n",
    "\n",
    "    print(f\"Batch requests saved to {filename}.jsonl\")\n",
    "\n",
    "# Example of calling the function\n",
    "data = pd.DataFrame({\n",
    "    'system_message': [\"You are a helpful assistant.\", \"You are an unhelpful assistant.\"],\n",
    "    'user_message': [\"Hello world!\", \"Hello world!\"],\n",
    "    'assistant_message': [\"How can I assist you today?\", \"I'm not sure how to help.\"]\n",
    "})\n",
    "generate_batch_examples(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Assuming parsed_data_df is your DataFrame with columns 'prompt' and 'caption'\n",
    "def process_in_batches(df, batch_size):\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        # Extract the current batch as a DataFrame slice\n",
    "        batch_df = df[i:i+batch_size]\n",
    "        \n",
    "        # Create 'prompt_caption' for the batch\n",
    "        batch_df['prompt_caption'] = batch_df['prompt'] + \"\\n\\n\" + batch_df['caption']\n",
    "        \n",
    "        # Process each prompt in the current batch and create a list of dictionaries\n",
    "        results = [{\n",
    "            \"index\": index,  # Add an index to keep track of the original row number\n",
    "            'prompt': row['prompt'], \n",
    "            'caption': row['caption'],\n",
    "            'prompt_caption': row['prompt_caption'],\n",
    "            'category': chat_service.generate_category(row['prompt_caption'])\n",
    "        } for index, row in batch_df.iterrows()]\n",
    "        \n",
    "        # Convert the results to a DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Optionally: save DataFrame to a file, database, etc.\n",
    "        results_df.to_csv(f'batch_{i//batch_size}.csv', index=False)  # Example to save to CSV\n",
    "        \n",
    "        print(f\"Processed batch {i//batch_size + 1}\")\n",
    "        print(results_df)\n",
    "        \n",
    "        # Sleep for 10 seconds before processing the next batch\n",
    "        time.sleep(10)\n",
    "\n",
    "# Call the function with the entire DataFrame and a batch size of 10\n",
    "process_in_batches(parsed_data_df, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = [{'prompt_caption': prompt, 'category': chat_service.generate_category(prompt)} for prompt in prompt_caption]\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data  = parsed_data_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Optional, Tuple\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# from app.services.cloud_service import CloudManager\n",
    "# from app.callbacks.streaming import StreamingHandler\n",
    "# from app.response.system import ReplyChainSystem\n",
    "# from app.services.chat_service import AI\n",
    "# from app.chain_tree.schemas import Chain\n",
    "# import uuid\n",
    "\n",
    "# class PromptService:\n",
    "#     def __init__(self,\n",
    "#                  model_name: Optional[str],\n",
    "#                  callback: Optional[StreamingHandler] = None,\n",
    "#                  credentials: Optional[dict] = None,\n",
    "#                  segment_delimiter: Optional[str] = \".\",\n",
    "#                  name: Optional[str] = None,\n",
    "#                  technique: Optional[object] = None,\n",
    "#                  upload: Optional[bool] = None,\n",
    "#                  path: Optional[str] = None,\n",
    "#                  with_responses: Optional[bool] = False,\n",
    "#                  storage: Optional[str] = None,\n",
    "#                  max_tokens: Optional[int] = 4096,\n",
    "#                  target_tokens: Optional[int] = 16385,\n",
    "#                  verbose: Optional[bool] = False,\n",
    "#                  create: Optional[bool] = False,\n",
    "#                  play: Optional[bool] = False,\n",
    "#                  show: Optional[bool] = False,\n",
    "#                  process_media: Optional[bool] = False,\n",
    "#                  internal: Optional[bool] = False,\n",
    "#                  audio_func: Optional[bool] = False,\n",
    "#                  stop: Optional[str] = None,\n",
    "#                  provider: Optional[str] = \"openai\",\n",
    "#                  convert: Optional[bool] = False,\n",
    "#                  subdirectory: Optional[str] = None):\n",
    "        \n",
    "#         self.stop = stop\n",
    "#         self.name = name\n",
    "#         self.path = path\n",
    "#         self.build = create\n",
    "#         self.upload = upload\n",
    "#         self.verbose = verbose\n",
    "#         self.convert = convert\n",
    "#         self.storage = storage\n",
    "#         self.provider = provider\n",
    "#         self.subdirectory = subdirectory\n",
    "#         self.with_responses = with_responses\n",
    "\n",
    "#         if subdirectory is None:\n",
    "#             subdirectory = str(uuid.uuid4())\n",
    "\n",
    "#         self.subdirectory = subdirectory\n",
    "\n",
    "#         self.prompt_manager = CloudManager(credentials=credentials, directory=storage)\n",
    "\n",
    "#         self.reply_chain_system = ReplyChainSystem(\n",
    "#             name=name,\n",
    "#             register_synthesis_technique=technique,\n",
    "#             verbose=verbose,\n",
    "#         )\n",
    "#         self.generator = Generator()\n",
    "#         self.callback = (\n",
    "#             callback\n",
    "#             if callback\n",
    "#             else StreamingHandler(\n",
    "#                 segment_delimiter=segment_delimiter,\n",
    "#             )\n",
    "#         )\n",
    "#         self.chat = AI(\n",
    "#             storage=storage,\n",
    "#             callbacks=[self.callback],\n",
    "#             model_name=model_name,\n",
    "#             prompt_manager=self.prompt_manager,\n",
    "#             max_tokens=max_tokens,\n",
    "#             create=create,\n",
    "#             play=play,\n",
    "#             process_media=process_media,\n",
    "#             internal=internal,\n",
    "#             show=show,\n",
    "#             audio_func=audio_func,\n",
    "#             provider=provider,\n",
    "#             subdirectory=subdirectory,\n",
    "#             target_tokens=target_tokens,\n",
    "#         )\n",
    "\n",
    "#     async def generate_prompt_parts(\n",
    "#         self,\n",
    "#         prompt: Optional[str] = None,\n",
    "#         response: Optional[str] = None,\n",
    "#         conversation_id: Optional[str] = None,\n",
    "#         use_basic_truncation: Optional[bool] = False,\n",
    "#         **kwargs,\n",
    "#     ) -> Chain:\n",
    "#         conversation_history = await self.reply_chain_system.prepare_conversation_history(\n",
    "#             prompt, response, **kwargs\n",
    "#         )\n",
    "\n",
    "#         truncated_history = self.chat._process_conversation_history(\n",
    "#             conversation_history,\n",
    "#             prompt=prompt,\n",
    "#             use_basic_truncation=use_basic_truncation,\n",
    "#             verbose=self.verbose,\n",
    "#         )\n",
    "\n",
    "#         return await self.chat(truncated_history, conversation_id, self.stop)\n",
    "\n",
    "#     async def _create_prompt_and_embedding(\n",
    "#         self,\n",
    "#         text: str,\n",
    "#         conversation_id: str,\n",
    "#         prompt: Optional[str] = None,\n",
    "#         upload: Optional[bool] = None,\n",
    "#         **kwargs,\n",
    "#     ) -> None:\n",
    "#         embedding = await self.chat.generate_embeddings(text)\n",
    "#         await self.prompt_manager.create_prompt(\n",
    "#             prompt=prompt,\n",
    "#             prompt_parts=text.split(\"\\n\\n\"),\n",
    "#             id=conversation_id,\n",
    "#             embedding=embedding,\n",
    "#             upload=upload,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "\n",
    "#     async def generate_prompt(\n",
    "#         self,\n",
    "#         prompt: Optional[str] = None,\n",
    "#         response: Optional[str] = None,\n",
    "#         **kwargs,\n",
    "#     ) -> str:\n",
    "#         text = await self.generator._generic_creation(\n",
    "#             prompt=prompt,\n",
    "#             response=response,\n",
    "#             generation_function=self.generate_prompt_parts,\n",
    "#             creation_function=self._create_prompt_and_embedding,\n",
    "#             upload=self.upload,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "#         return text\n",
    "\n",
    "#     async def run_thread(self, prompt, **kwargs) -> str:\n",
    "#         return await self.generator._generic_prompt_creation(\n",
    "#             prompt=prompt,\n",
    "#             generation_function=self.generate_prompt,\n",
    "#             creation_function=self._create_prompt_and_embedding,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "\n",
    "#     async def run_chat(self, **kwargs) -> str:\n",
    "#         return await self.generator.run_chat(\n",
    "#             generate_prompt=self.generate_prompt_parts,\n",
    "#             chat=self.chat,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "\n",
    "#     async def _create_future_tasks(\n",
    "#         self,\n",
    "#         executor: ThreadPoolExecutor,\n",
    "#         prompt: str,\n",
    "#         answer: str = None,\n",
    "#         parent_id: str = None,\n",
    "#         answer_split: bool = False,\n",
    "#         mode: str = \"run\",\n",
    "#         **kwargs, \n",
    "#     ):\n",
    "#         futures = []\n",
    "#         if answer_split:\n",
    "#             responses = answer.split(\"\\n\\n\")\n",
    "#         else:\n",
    "#             responses = [answer]\n",
    "\n",
    "#         for index, response in enumerate(responses):\n",
    "#             task_parent_id = f\"{parent_id}_{index}\" if answer_split else parent_id\n",
    "\n",
    "#             if mode == \"run\":\n",
    "#                 future = executor.submit(self.run_thread, prompt=prompt, **kwargs)\n",
    "\n",
    "#             elif mode == \"run_chat\":\n",
    "#                 future = executor.submit(self.run_chat, **kwargs)\n",
    "\n",
    "#             else:\n",
    "#                 future = executor.submit(\n",
    "#                     self.generate_prompt,\n",
    "#                     prompt,\n",
    "#                     response,\n",
    "#                     **kwargs,\n",
    "#                 )\n",
    "#             futures.append(future)\n",
    "#         return futures\n",
    "\n",
    "#     async def _get_unique_prompt(self, example_pairs: List, generated_prompts: set):\n",
    "#         parent_id = str(uuid.uuid4())\n",
    "#         prompt, answer = example_pairs.pop()\n",
    "#         while (prompt, answer, parent_id) in generated_prompts:\n",
    "#             prompt, answer, parent_id = example_pairs.pop()\n",
    "#         generated_prompts.add((prompt, answer, parent_id))\n",
    "#         return prompt, answer, parent_id\n",
    "\n",
    "#     async def run_interactive(\n",
    "#         self,\n",
    "#         mode: str = \"run\",\n",
    "#         answer_split: bool = False,\n",
    "#         max_workers: int = 1,\n",
    "#         batch_size: int = 4,\n",
    "#         example_pairs: Optional[List[Tuple[str, str]]] = None,\n",
    "#         with_responses: Optional[bool] = True,\n",
    "#     ) -> None:\n",
    "#         num_prompts = len(example_pairs)\n",
    "#         generated_prompts = set()\n",
    "#         total_batches = (num_prompts + batch_size - 1) // batch_size\n",
    "#         valid_count = 0\n",
    "\n",
    "#         for batch_num in range(total_batches):\n",
    "#             print(f\"Generating batch {batch_num + 1} of {total_batches}\")\n",
    "\n",
    "#             with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#                 futures = []\n",
    "\n",
    "#                 for _ in range(min(batch_size, num_prompts - valid_count)):\n",
    "#                     prompt, answer, parent_id = await self._get_unique_prompt(\n",
    "#                         example_pairs, generated_prompts\n",
    "#                     )\n",
    "#                     if with_responses:\n",
    "#                         futures.extend(\n",
    "#                             self._create_future_tasks(\n",
    "#                                 executor,\n",
    "#                                 prompt,\n",
    "#                                 answer,\n",
    "#                                 parent_id,\n",
    "#                                 answer_split,\n",
    "#                                 mode,\n",
    "#                             )\n",
    "#                         )\n",
    "#                     else:\n",
    "#                         futures.extend(\n",
    "#                             self._create_future_tasks(\n",
    "#                                 executor,\n",
    "#                                 prompt,\n",
    "#                                 None,\n",
    "#                                 parent_id,\n",
    "#                                 answer_split,\n",
    "#                                 mode,\n",
    "#                             )\n",
    "#                         )\n",
    "\n",
    "#                 for future in futures:\n",
    "#                     await future.result()\n",
    "\n",
    "#             valid_count += len(futures)\n",
    "\n",
    "#         print(f\"Generated {valid_count} prompts.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
